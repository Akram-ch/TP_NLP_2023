{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f84674-385b-427b-b682-9287dc596c2a",
   "metadata": {},
   "source": [
    "# Acoustic and textual data analysis with Python\n",
    "\n",
    "This practical work focuses on Python programming language's capabilities in analyzing textual and acoustic data. This practical work will cover the basics of Python programming, key concepts in acoustics and textual data, and how to use Python to analyze these data. By the end of this practical work, you will be able to confidently use Python to analyse acoustics and textual data. More specifically here you will learn the following:\n",
    "\n",
    "- Python objects and how to effectively use them\n",
    "- Web-scraping for text, tokenisation and stemming of text, and basic textual feature extraction (tf-idf)\n",
    "- Audio encodings, conversion of the encodings, and analysis of spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6d87d-4102-4a60-8769-f5dfcccc665d",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "Before doing anything, we first need to install the packages that are going to be used in this practical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbae20b-6e32-4d44-8e73-6d5c1e2448dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4==4.11.2\n",
    "!pip install nltk==3.8.1\n",
    "!pip install numpy==1.24.1\n",
    "!pip install pandas==1.5.3\n",
    "!pip install -U scikit-learn==1.2.1\n",
    "!pip install ffmpeg-python==0.2.0\n",
    "!pip install scipy==1.10.0\n",
    "!pip install matplotlib==3.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560467e9-fc65-4585-b5bb-0b7ada9fda68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python basics\n",
    "\n",
    "It is essential to have a basic understanding of the fundamentals of Python programming for this practical work. Here, you should be familiar with the following concepts:\n",
    "\n",
    "- Objects (every entity in python refers to an object)\n",
    "- Variables, functions and control flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8e29a-5463-40ba-aee5-67b643c60b58",
   "metadata": {},
   "source": [
    "### Exercise 1: Variables\n",
    "In the code below, we first want the dictionary `b` to have the same values of the dictionary `a`, but be different for the item `0`. However, after writing the function `change_value_for_zero` to do this task, we realise the results are not as expected. How can we fix the code below? Please also add your explanation of the problem as the `explain` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1ac45-6fd1-42fe-8876-fd3ffb466cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def change_value_for_zero(inp):\n",
    "    out = inp\n",
    "    out[0] = \"d\"\n",
    "    return out\n",
    "\n",
    "a = {0:\"a\", 1:\"b\", 2:\"c\"}\n",
    "b = change_value_for_zero(a)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"What caused the problem:\", explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45067f09-ee77-47c3-b968-333e84bd9a4e",
   "metadata": {},
   "source": [
    "### Exercise 2: Objects\n",
    "Here we want to define Python classes for a classroom and its students, to make adding new students to the class easier and more automated. However, we simply do not know how the attributes we consider for each student now will change over time. For example, at first we might only be interested in the student's name, but some time later we might also want to consider the student's hometown. Therefore, the script we write must allow for this variability.\n",
    "\n",
    "First, we want to write a class to represent the students of different classes called `classroom`. However, here it is used to represent the students of an `NLP class` (look at the `main` function below). We also want this class to be able to write a csv file. The csv file must contain the attributes (id, name, etc.) as colums, and each row represents a student.\n",
    "\n",
    "**Attention**: the `main` function shall not be changed. The attributes (name, age, town, etc.) shall not be named explicitly in the `classroom` class. The idea is to be able to reuse the `classroom` class for other purposes, avoid `hard-coding` ([see here](https://en.wikipedia.org/wiki/Hard_coding)), and keep the codes `extensible` ([see here](https://en.wikipedia.org/wiki/Extensible_programming))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c78fc-a645-4e4f-be87-60ea01f15d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class classroom():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def add_student(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def write_to_csv(self, path):\n",
    "        pass\n",
    "\n",
    "def main():\n",
    "    nlp_class = classroom()\n",
    "    nlp_class.add_student(name=\"François\")\n",
    "    nlp_class.add_student(name=\"Benjamin\")\n",
    "    nlp_class.add_student(name=\"Didier\", age=23)\n",
    "    nlp_class.add_student(name=\"Fabien\", town=\"Paris\")\n",
    "    csv_path = \"./students.csv\"\n",
    "    nlp_class.write_to_csv(csv_path)\n",
    "    os.system(f\"cat {csv_path}\")\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a4ec2-5bfb-42ef-9098-b0c76f1635ad",
   "metadata": {},
   "source": [
    "## Text analysis with python\n",
    "\n",
    "Text analysis is an increasingly important tool for researchers to gain insight from large amounts of data. Python for text analysis has become extremely popular lately, due to its flexibility and wide range of libraries and packages. In this section we discuss the following:\n",
    "\n",
    "- web scraping and extracting a specific text\n",
    "- tokenisation, and stemming the text\n",
    "- loading text-based datasets\n",
    "- basic feature extraction for dataset analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f84c0-3a8d-4c21-b058-f0068da6756e",
   "metadata": {},
   "source": [
    "### Exercise 3: Text analysis basics\n",
    "\n",
    "Here, we first want to find the `description` in the `meta content` of [UGA website's main page](https://www.univ-grenoble-alpes.fr). Then, we want to automatically analyse its contents. For this purpose, we would like to first tokenise the text, and then find the stems of each word. See [here](https://medium.com/@jeevanchavan143/nlp-tokenization-stemming-lemmatization-bag-of-words-tf-idf-pos-7650f83c60be) and [here](https://dair.ai/notebooks/nlp/2020/03/19/nlp_basics_tokenization_segmentation.html) for more information on the mentioned concepts, as well as practical application.\n",
    "\n",
    "**help**: You can use the `requests` package to `GET` the contents, and `BeautifulSoup` package to parse the `html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277c9a7-d549-497c-a782-364a28dedb35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "nltk.download('punkt') # to download the tokenisers\n",
    "\n",
    "def get_text(URL):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return txt\n",
    "\n",
    "def tokenise_text(txt):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return tokenized_txt\n",
    "\n",
    "\n",
    "def stem_text(tokenized_txt):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return stemmed_txt\n",
    "\n",
    "def main():\n",
    "    txt = get_text(\"https://www.univ-grenoble-alpes.fr\")\n",
    "    tokenized_txt = tokenise_text(txt)\n",
    "    stemmed_txt = stem_text(tokenized_txt)\n",
    "    print(\"original text:\", txt)\n",
    "    print(\"tokenized text:\", tokenized_txt)\n",
    "    print(\"stemmed text:\", stemmed_txt)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f46f49-d8be-4bc9-a853-60ca2809db89",
   "metadata": {},
   "source": [
    "### Excercise 4: Loading datasets\n",
    "\n",
    "In this exercise we will learn how to load and analyse textual datasets. Here we will focus on a sentiment analysis dataset, called `Allociné`, which consists of reviews of French television series. These reviews can be positive (labelled `1`) or negative (labelled `0`). Here are your tasks for this excercise:\n",
    "\n",
    "- Download the corpus\n",
    "- `Allociné` is divided into three partitions of `train`, `dev`, and `test`. Write a script to load the `train` partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddf0db-2ec2-4e36-b536-5ee78e48eee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading the dataset\n",
    "import os\n",
    "dl_path = \"http://sentiment.nlproc.org/sentiment-dataset-fr.zip\"\n",
    "os.system(f\"wget {dl_path}\")\n",
    "os.system(f\"unzip ./sentiment-dataset-fr.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202628b4-ca91-4eef-b531-b19e66a95b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_all_dataset(path):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return train_data\n",
    "\n",
    "def get_positive_samples(data):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return positives\n",
    "    \n",
    "def get_negative_samples(data):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return negatives\n",
    "\n",
    "train_data = get_all_dataset(\"./fr/train.tsv\")\n",
    "train_data_positives = get_positive_samples(train_data)\n",
    "train_data_negatives = get_negative_samples(train_data)\n",
    "\n",
    "print(\"An example of a negative comment:\\n\", train_data_negatives[0,1])\n",
    "print(\"An example of a positive comment:\\n\", train_data_positives[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d10d0-7aa4-4dde-829f-9031cd5cc6dd",
   "metadata": {},
   "source": [
    "### Excercise 5: Basic feature extraction\n",
    "\n",
    "After loading the dataset, we want to see if we can somehow use each word to automatically distinguish between positive and negative comments. Automatic detection of negativity/positivity from text is called sentiment analysis, and it has many applications, especially in marketing campaigns and product analysis. The first step of such task, is to represent the text numerically in a way that is useful for sentiment analysis. Traditionally, a method called tf-idf is used to extract word-based features from different documents. tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus, and is usually used in information retrieval (from [wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)). [See here](https://monkeylearn.com/blog/what-is-tf-idf/) for more information on tf-idf and its applications for machine learning.\n",
    "\n",
    "In this excercise, we would like to extract numerical vectors to represent each word using the tf–idf method. In the code section below, \n",
    "\n",
    "- Using `TfidfVectorizer`, write a function that would get the training partition of the `Allociné` corpus, and applies the tf–idf.\n",
    "- Transform the `Allociné`'s training partition to represent the stems of each word. Then apply tf-idf to the \"stemmed\" corpus.\n",
    "- Then, explain which set of tf-idf features you prefer to use? the one from the original text of `Allociné` corpus, or its stemmed version? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c2e23-61b3-4635-af13-e3384df6096a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_vectorizer(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    _ = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer\n",
    "\n",
    "def get_tfidf_of_words(text, vectorizer):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return tf_idfs    \n",
    "\n",
    "corpus = train_data[:,1]\n",
    "vectorizer = get_vectorizer(corpus)\n",
    "tokenised_corpus = [tokenise_text(txt) for txt in corpus]\n",
    "stemmed_corpus = [stem_text(txt) for txt in tokenised_corpus]\n",
    "vectorizer_stemmed = get_vectorizer(stemmed_corpus)\n",
    "\n",
    "example = corpus[0]\n",
    "example_stemmed = stemmed_corpus[0]\n",
    "tf_idfs = get_tfidf_of_words(example, vectorizer)\n",
    "tf_idfs_stemmed = get_tfidf_of_words(example_stemmed, vectorizer_stemmed)\n",
    "\n",
    "print(\"original text: \", example)\n",
    "print(\"Tf-idf of the original text: \", tf_idfs)\n",
    "print(\"stemmed text: \", example_stemmed)\n",
    "print(\"Tf-idf of the stemmed text: \", tf_idfs_stemmed)\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"Which set of features would you choose and why?\", explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe12931-f033-4b17-ad2d-6913a35c0d78",
   "metadata": {},
   "source": [
    "### Excercise 6: Text analysis\n",
    "\n",
    "Here, we want to see if there is a correlation between the `tf-idf` of each word and the sentiment labels. Fill in the function below to get the first `30` words with the most `tf-idf` values, for the positive and negative comments separately. Do you see a correlation between the words used and the sentiment labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ff439-d21b-4062-9572-87eb631d5339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_most_repeated(data, vectorizer):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return words\n",
    "    \n",
    "positive_words = get_most_repeated(train_data_positives, vectorizer)\n",
    "negative_words = get_most_repeated(train_data_negatives, vectorizer)\n",
    "print(\"positive_words:\", positive_words)\n",
    "print(\"negative_words:\", negative_words)\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"Do you see a correlation of the words with the most tf-idf values, and sentiment labels?\", explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a43178-f529-4199-b529-4a9b830ed85a",
   "metadata": {},
   "source": [
    "## Acoustic signal analysis with Python\n",
    "\n",
    "Acoustic signal analysis is a powerful tool used to extract meaningful information from acoustic recordings. Here, we cover the following topics:\n",
    "\n",
    "- Different audio encodings and conversion\n",
    "- Analysing acoustic recordings with spectrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45e90f-89a0-4684-b3d5-f7b7652b7f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading two audio recordings to analyse\n",
    "dl_path = 'https://cdn.pixabay.com/download/audio/2023/01/09/audio_baaa3cfec7.mp3?filename=acoustic-guitar-loop-f-91bpm-132687.mp3'\n",
    "os.system(f\"curl {dl_path} --compressed -o guitar.mp3\")\n",
    "dl_path = \"https://cdn.pixabay.com/download/audio/2022/03/15/audio_3e683188e5.mp3?filename=tooheavy-77056.mp3\"\n",
    "os.system(f\"curl {dl_path} --compressed -o speech.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b58e9-3e6a-4739-af0e-1401a522cbb9",
   "metadata": {},
   "source": [
    "### Excercise 7: Audio encodings\n",
    "\n",
    "Below, write a script to convert the `.mp3` files to `.wav` files formatted as `PCM signed 16-bit little-endian`. We would also like to have the file as `mono`, so that they only contain one channel. Last but not the least, change the sampling frequency of the files to `16000 Hz`. After writting the script, explain the questions asked in the code section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633f6f2-3b51-4b3b-99bd-7c63da1f70b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_audio(input_path, output_path):\n",
    "    args = 'Write your code here'\n",
    "    os.system(f'ffmpeg -i {input_path} {args} -y {output_path}')\n",
    "\n",
    "convert_audio(\"guitar.mp3\", \"guitar_converted.wav\")\n",
    "convert_audio(\"speech.mp3\", \"speech_converted.wav\")\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"What is the difference between mp3, and PCM formatted wav file?\", explain)\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"What is the sampling frequency, its difference with bit-rate?\", explain)\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"Analysing speech signals, and building machine learning models to recognise human speech are often done with 16KHz sampled audio files. Why do you think that is the case ?\", explain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaaa064-41b9-4ba7-9fd4-8f44bbcc1d1f",
   "metadata": {},
   "source": [
    "### Excercise 8: Normalisation\n",
    "\n",
    "It is a common practice to normalise the wav signal to contain values between zero and one. Write a very short script (can be one line) to normalise the wav file after `reading` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d6227-9e9c-4c59-ae03-472d53307692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "\n",
    "def normalise_signal(sig):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return sig_normed\n",
    "\n",
    "(rate,sig) = wav.read(\"guitar_converted.wav\")\n",
    "sig_normed = normalise_signal(sig)\n",
    "print(\"original signal values:\", sig[995:1000])\n",
    "print(\"normalised signal values\", sig_normed[995:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c2ccc-bf9c-4c60-a5ef-7139631e67ba",
   "metadata": {},
   "source": [
    "### Excercise 9: Spectrograms\n",
    "\n",
    "Below, we use the `matplotlib` package to plot the spectrograms of the two acoustic signals. A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time (from [wikipedia](https://en.wikipedia.org/wiki/Spectrogram)). After running the code below to plot the spectrograms, answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3560d-fefa-4907-b318-e4b868130af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(rate,sig) = wav.read(\"guitar_converted.wav\")\n",
    "spc, f, t, img = plt.specgram(sig[:rate*3], Fs=rate, NFFT=512) #spc, f, t, img\n",
    "plt.show()\n",
    "\n",
    "(rate,sig) = wav.read(\"speech_converted.wav\")\n",
    "spc, f, t, img = plt.specgram(sig[:rate*4], Fs=rate, NFFT=512) #spc, f, t, img\n",
    "plt.show()\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"From the spectrograms, how can you tell which one is human speech, and which one is guitar sound?\", explain)\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"Try chaning the value of NFFT, what change do you observe? why is that?\", explain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a38e0-bc1f-4ad1-afad-55958e3c489e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this practical work, the basics of acoustic and textual data analysis with Python have been explored. First, the fundamentals of Python were covered, focusing on classes and how to write clean codes. Then, the basics of text analysis were discussed, including downloading datasets and extracting basic features such as `tf-idf`. Finally, acoustic signal analysis with Python was discussed, focusing on audio encodings, conversion of the encodings, and acoustic analysis of different sounds with `spectrograms`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
