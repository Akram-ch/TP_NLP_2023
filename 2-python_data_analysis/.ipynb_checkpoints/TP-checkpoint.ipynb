{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f84674-385b-427b-b682-9287dc596c2a",
   "metadata": {},
   "source": [
    "# Acoustic and textual data analysis with Python\n",
    "\n",
    "This practical work focuses on Python programming language's capabilities in analyzing textual and acoustic data. This practical work will cover the basics of Python programming, key concepts in acoustics and textual data, and how to use Python to analyze these data. By the end of this practical work, you will be able to confidently use Python to analyse acoustics and textual data. More specifically here you will learn the following:\n",
    "\n",
    "- Python objects and how to effectively use them\n",
    "- Web-scraping for text, tokenisation and stemming of text, and basic textual feature extraction (tf-idf)\n",
    "- Audio encodings, conversion of the encodings, and analysis of spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6d87d-4102-4a60-8769-f5dfcccc665d",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "Before doing anything, we first need to install the packages that are going to be used in this practical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fbae20b-6e32-4d44-8e73-6d5c1e2448dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4==4.11.2 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from beautifulsoup4==4.11.2) (2.3.2.post1)\n",
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from nltk==3.8.1) (2022.10.31)\n",
      "Requirement already satisfied: joblib in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from nltk==3.8.1) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from nltk==3.8.1) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from nltk==3.8.1) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from click->nltk==3.8.1) (0.4.6)\n",
      "Requirement already satisfied: numpy==1.24.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (1.24.1)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from pandas==1.5.3) (1.24.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from pandas==1.5.3) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from pandas==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn==1.2.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from scikit-learn==1.2.1) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from scikit-learn==1.2.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from scikit-learn==1.2.1) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from scikit-learn==1.2.1) (1.24.1)\n",
      "Requirement already satisfied: ffmpeg-python==0.2.0 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from ffmpeg-python==0.2.0) (0.18.3)\n",
      "Requirement already satisfied: scipy==1.10.0 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from scipy==1.10.0) (1.24.1)\n",
      "Requirement already satisfied: matplotlib==3.6.3 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (3.6.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (4.38.0)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from matplotlib==3.6.3) (23.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akram\\anaconda3\\envs\\tp_nlp_2023\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.6.3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4==4.11.2\n",
    "!pip install nltk==3.8.1\n",
    "!pip install numpy==1.24.1\n",
    "!pip install pandas==1.5.3\n",
    "!pip install -U scikit-learn==1.2.1\n",
    "!pip install ffmpeg-python==0.2.0\n",
    "!pip install scipy==1.10.0\n",
    "!pip install matplotlib==3.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560467e9-fc65-4585-b5bb-0b7ada9fda68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python basics\n",
    "\n",
    "It is essential to have a basic understanding of the fundamentals of Python programming for this practical work. Here, you should be familiar with the following concepts:\n",
    "\n",
    "- Objects (every entity in python refers to an object)\n",
    "- Variables, functions and control flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8e29a-5463-40ba-aee5-67b643c60b58",
   "metadata": {},
   "source": [
    "### Exercise 1: Variables\n",
    "In the code below, we first want the dictionary `b` to have the same values of the dictionary `a`, but be different for the item `0`. However, after writing the function `change_value_for_zero` to do this task, we realise the results are not as expected. How can we fix the code below? Please also add your explanation of the problem as the `explain` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a1ac45-6fd1-42fe-8876-fd3ffb466cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: {0: 'a', 1: 'b', 2: 'c'}\n",
      "b: {0: 'd', 1: 'b', 2: 'c'}\n",
      "What caused the problem: the variables a and b are both references to the same dictionary object, therefore any changes made to the dictionnary will appear on both variables\n"
     ]
    }
   ],
   "source": [
    "def change_value_for_zero(inp):\n",
    "    out = inp\n",
    "    out[0] = \"d\"\n",
    "    return out\n",
    "\n",
    "a = {0:\"a\", 1:\"b\", 2:\"c\"}\n",
    "b = dict(a)\n",
    "b = change_value_for_zero(b)\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "\n",
    "explain = \"the variables a and b are both references to the same dictionary object, therefore any changes made to the dictionnary will appear on both variables\"\n",
    "print(\"What caused the problem:\", explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45067f09-ee77-47c3-b968-333e84bd9a4e",
   "metadata": {},
   "source": [
    "### Exercise 2: Objects\n",
    "Here we want to define Python classes for a classroom and its students, to make adding new students to the class easier and more automated. However, we simply do not know how the attributes we consider for each student now will change over time. For example, at first we might only be interested in the student's name, but some time later we might also want to consider the student's hometown. Therefore, the script we write must allow for this variability.\n",
    "\n",
    "First, we want to write a class to represent the students of different classes called `classroom`. However, here it is used to represent the students of an `NLP class` (look at the `main` function below). We also want this class to be able to write a csv file. The csv file must contain the attributes (id, name, etc.) as colums, and each row represents a student.\n",
    "\n",
    "**Attention**: the `main` function shall not be changed. The attributes (name, age, town, etc.) shall not be named explicitly in the `classroom` class. The idea is to be able to reuse the `classroom` class for other purposes, avoid `hard-coding` ([see here](https://en.wikipedia.org/wiki/Hard_coding)), and keep the codes `extensible` ([see here](https://en.wikipedia.org/wiki/Extensible_programming))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872c78fc-a645-4e4f-be87-60ea01f15d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "class classroom():\n",
    "    def __init__(self):\n",
    "        self.student_list = {}\n",
    "        self.student_id = 1\n",
    "    \n",
    "    def add_student(self, **kwargs):\n",
    "        self.student_list[self.student_id] = kwargs\n",
    "        self.student_id += 1\n",
    "    \n",
    "    def write_to_csv(self, path):\n",
    "       \n",
    "        with open(path, mode='w') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for id in self.student_list :\n",
    "                list = []\n",
    "                for att in self.student_list[id]:\n",
    "                    list.append(self.student_list[id][att])\n",
    "                writer.writerow(list)\n",
    "\n",
    "def main():\n",
    "    nlp_class = classroom()\n",
    "    nlp_class.add_student(name=\"François\")\n",
    "    nlp_class.add_student(name=\"Benjamin\")\n",
    "    nlp_class.add_student(name=\"Didier\", age=23)\n",
    "    nlp_class.add_student(name=\"Fabien\", town=\"Paris\")\n",
    "    csv_path = \"./students.csv\"\n",
    "    nlp_class.write_to_csv(csv_path)\n",
    "    os.system(f\"cat {csv_path}\")\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a4ec2-5bfb-42ef-9098-b0c76f1635ad",
   "metadata": {},
   "source": [
    "## Text analysis with python\n",
    "\n",
    "Text analysis is an increasingly important tool for researchers to gain insight from large amounts of data. Python for text analysis has become extremely popular lately, due to its flexibility and wide range of libraries and packages. In this section we discuss the following:\n",
    "\n",
    "- web scraping and extracting a specific text\n",
    "- tokenisation, and stemming the text\n",
    "- loading text-based datasets\n",
    "- basic feature extraction for dataset analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f84c0-3a8d-4c21-b058-f0068da6756e",
   "metadata": {},
   "source": [
    "### Exercise 3: Text analysis basics\n",
    "\n",
    "Here, we first want to find the `description` in the `meta content` of [UGA website's main page](https://www.univ-grenoble-alpes.fr). Then, we want to automatically analyse its contents. For this purpose, we would like to first tokenise the text, and then find the stems of each word. See [here](https://medium.com/@jeevanchavan143/nlp-tokenization-stemming-lemmatization-bag-of-words-tf-idf-pos-7650f83c60be) and [here](https://dair.ai/notebooks/nlp/2020/03/19/nlp_basics_tokenization_segmentation.html) for more information on the mentioned concepts, as well as practical application.\n",
    "\n",
    "**help**: You can use the `requests` package to `GET` the contents, and `BeautifulSoup` package to parse the `html`. Also, you can use the `nltk` package for tokenisation and stemming the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c277c9a7-d549-497c-a782-364a28dedb35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous vous offrons tout l’éventail des disciplines universitaires en formation initiale et tout au long de la vie : de la littérature à la physique des particules en passant par l’ingénierie, l'architecture, la politique, la sociologie, la médecine... De quoi construire des parcours de formation riches, flexibles, adaptés aux aspirations de chacun et permettant un accès rapide à l'emploi.\n",
      "\n",
      "original text: Nous vous offrons tout l’éventail des disciplines universitaires en formation initiale et tout au long de la vie : de la littérature à la physique des particules en passant par l’ingénierie, l'architecture, la politique, la sociologie, la médecine... De quoi construire des parcours de formation riches, flexibles, adaptés aux aspirations de chacun et permettant un accès rapide à l'emploi.\n",
      "\n",
      "tokenized text: Nous vous offrons tout l ’ éventail des disciplines universitaires en formation initiale et tout au long de la vie : de la littérature à la physique des particules en passant par l ’ ingénierie , l'architecture , la politique , la sociologie , la médecine ... De quoi construire des parcours de formation riches , flexibles , adaptés aux aspirations de chacun et permettant un accès rapide à l'emploi .\n",
      "\n",
      "stemmed text: nous vous offron tout l ’ éventail de disciplin universitair en format initial et tout au long de la vi : de la littératur à la physiqu de particul en pass par l ’ ingénier , l'architectur , la polit , la sociolog , la médecin ... de quoi construir de parcour de format rich , flexibl , adapt aux aspir de chacun et permet un acces rapid à l'emploi .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.stem.snowball import FrenchStemmer \n",
    "nltk.download('punkt') # to download the tokenisers\n",
    "\n",
    "def get_text(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(\"meta\", {\"name\":\"description\"})\n",
    "    txt = results.prettify()\n",
    "    txt = txt.replace(\"<meta content=\\\"\", \"\")\n",
    "    txt = txt.replace(\"\\\" name=\\\"description\\\"/>\", \"\")\n",
    "    print(txt)\n",
    "    return txt\n",
    "\n",
    "def tokenise_text(txt):\n",
    "    ''' here we want the output to be a text that puts a space between each token (not a list)\n",
    "    Write your code here\n",
    "    '''\n",
    "    tokenized_txt = \" \".join(wt(txt))\n",
    "    return tokenized_txt\n",
    "\n",
    "def stem_text(tokenized_txt):\n",
    "    ''' here we want the output to be a text that puts a space between each stemmed token\n",
    "    Write your code here\n",
    "    '''\n",
    "    french_stemmer = FrenchStemmer()\n",
    "    text = tokenized_txt.split()\n",
    "    stemmed_list = [french_stemmer.stem(word) for word in text]\n",
    "    stemmed_txt = ' '.join(stemmed_list)\n",
    "    return stemmed_txt\n",
    "\n",
    "def main():\n",
    "    txt = get_text(\"https://www.univ-grenoble-alpes.fr\")\n",
    "    tokenized_txt = tokenise_text(txt)\n",
    "    stemmed_txt = stem_text(tokenized_txt)\n",
    "    print(\"original text:\", txt)\n",
    "    print(\"tokenized text:\", tokenized_txt + \"\\n\")\n",
    "    print(\"stemmed text:\", stemmed_txt + \"\\n\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f46f49-d8be-4bc9-a853-60ca2809db89",
   "metadata": {},
   "source": [
    "### Excercise 4: Loading datasets\n",
    "\n",
    "In this exercise we will learn how to load and analyse textual datasets. Here we will focus on a sentiment analysis dataset, called `Allociné`, which consists of reviews of French television series. These reviews can be positive (labelled `1`) or negative (labelled `0`). Here are your tasks for this excercise:\n",
    "\n",
    "- Download the corpus\n",
    "- `Allociné` is divided into three partitions of `train`, `dev`, and `test`. Write a script to load the `train` partition.\n",
    "- write a script for `get_all_dataset` function below to load training data of the downloaded corpus into the output. You can do it as a numpy array but you are free to do it as you wish.\n",
    "- write a script for `get_positive_samples` function below to get as input the output of the `get_all_dataset` output, and only output the comments that were positive\n",
    "- do the same as above for `get_positive_samples` function below, but this time we want the negative comments.\n",
    "- **Attention**: only the functions can be changed (indicated by `write your code here`), do not change other lines (except for the indices like `[0,1]`, however it is not recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddf0db-2ec2-4e36-b536-5ee78e48eee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading the dataset\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "dl_path = \"http://sentiment.nlproc.org/sentiment-dataset-fr.zip\"\n",
    "urllib.request.urlretrieve(dl_path, 'sentiment-dataset-fr.zip')\n",
    "with zipfile.ZipFile('sentiment-dataset-fr.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "#os.system(f\"wget {dl_path}\")\n",
    "#os.system(f\"unzip ./sentiment-dataset-fr.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "202628b4-ca91-4eef-b531-b19e66a95b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a negative comment:\n",
      "      score                                             review\n",
      "159      0  une série qui atteint le summum de la bêtise !...\n",
      "An example of a positive comment:\n",
      "      score                                             review\n",
      "760      1  une série qui montre les aspects négatifs de l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akram\\AppData\\Local\\Temp\\ipykernel_11460\\3189262476.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data.append(pd.DataFrame(train_data.columns))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_all_dataset(path):\n",
    "    train_data = pd.read_csv(path, sep='\\t')\n",
    "    new_columns = [\"score\", \"review\"]\n",
    "    train_data.append(pd.DataFrame(train_data.columns))\n",
    "    train_data.columns = new_columns\n",
    "    return train_data\n",
    "\n",
    "def get_positive_samples(data):\n",
    "    positives = data[data['score']==1]\n",
    "    return positives\n",
    "    \n",
    "def get_negative_samples(data):\n",
    "    negatives = data[data['score']==0]\n",
    "    return negatives\n",
    "\n",
    "train_data = get_all_dataset(\"./fr/train.tsv\")\n",
    "train_data_positives = get_positive_samples(train_data)\n",
    "train_data_negatives = get_negative_samples(train_data)\n",
    "\n",
    "print(\"An example of a negative comment:\\n\", train_data_negatives.sample(n=1))\n",
    "print(\"An example of a positive comment:\\n\", train_data_positives.sample(n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d10d0-7aa4-4dde-829f-9031cd5cc6dd",
   "metadata": {},
   "source": [
    "### Excercise 5: Basic feature extraction\n",
    "\n",
    "After loading the dataset, we want to see if we can somehow use each word to automatically distinguish between positive and negative comments. Automatic detection of negativity/positivity from text is called sentiment analysis, and it has many applications, especially in marketing campaigns and product analysis. The first step of such task, is to represent the text numerically in a way that is useful for sentiment analysis. Traditionally, a method called tf-idf is used to extract word-based features from different documents. tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus, and is usually used in information retrieval (from [wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)). [See here](https://monkeylearn.com/blog/what-is-tf-idf/) for more information on tf-idf and its applications for machine learning.\n",
    "\n",
    "In this excercise, we would like to extract numerical vectors to represent each word using the tf–idf method. In the code section below, \n",
    "\n",
    "- Using `TfidfVectorizer`, write a function that would get the training partition of the `Allociné` corpus, and applies the tf–idf. you can look at its documentation to see how to transform the text into tf-idf features. Be careful that the type of the output of this function can be different than a list or array, but one can use `.toarray()` to change the type to `numpy array` so that it is easier to work with.\n",
    "- Transform the `Allociné`'s training partition to represent the stems of each word. Then apply tf-idf to the \"stemmed\" corpus.\n",
    "- Then, explain which set of tf-idf features you prefer to use? the one from the original text of `Allociné` corpus, or its stemmed version? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "058c2e23-61b3-4635-af13-e3384df6096a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  série inintéressante banale le scénario est décevant et on ne rentre pas du tout dans la série . \n",
      "\n",
      "Tf-idf of the original text:  [[0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n",
      "  0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n",
      "  0.22941573 0.22941573 0.45883147 0.22941573]] \n",
      "\n",
      "stemmed text:  ser inintéress banal le scénario est décev et on ne rentr pas du tout dan la ser . \n",
      "\n",
      "Tf-idf of the stemmed text:  [[0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n",
      "  0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n",
      "  0.22941573 0.22941573 0.45883147 0.22941573]] \n",
      "\n",
      "Which set of tf-idf would you choose as features (from stemmed or original text) and why? While stemming can help capture the essence of a text and reduce the impact of noise and redundancy in the data, in the case of sentiment analysis it can be a bit counterproductive, as it may be important to distinguish between words that have different meanings depending on the tense or the conjugation, (eg love/loved)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_vectorizer(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    _ = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer\n",
    "\n",
    "def get_tfidf_of_words(text, vectorizer):\n",
    "    tf_idfs = vectorizer.fit_transform([text]).toarray()\n",
    "    return tf_idfs    \n",
    "\n",
    "corpus = train_data.review.values # you can also change this line\n",
    "vectorizer = get_vectorizer(corpus)\n",
    "tokenised_corpus = [tokenise_text(txt) for txt in corpus]\n",
    "stemmed_corpus = [stem_text(txt) for txt in tokenised_corpus]\n",
    "vectorizer_stemmed = get_vectorizer(stemmed_corpus)\n",
    "\n",
    "example = corpus[0]\n",
    "example_stemmed = stemmed_corpus[0]\n",
    "tf_idfs = get_tfidf_of_words(example, vectorizer)\n",
    "tf_idfs_stemmed = get_tfidf_of_words(example_stemmed, vectorizer_stemmed)\n",
    "\n",
    "print(\"original text: \", example , \"\\n\")\n",
    "print(\"Tf-idf of the original text: \", tf_idfs , \"\\n\")\n",
    "print(\"stemmed text: \", example_stemmed , \"\\n\")\n",
    "print(\"Tf-idf of the stemmed text: \", tf_idfs_stemmed, \"\\n\")\n",
    "\n",
    "explain = \"While stemming can help capture the essence of a text and reduce the impact of noise and redundancy in the data, in the case of sentiment analysis it can be a bit counterproductive, as it may be important to distinguish between words that have different meanings depending on the tense or the conjugation, (eg love/loved)\"\n",
    "print(\"Which set of tf-idf would you choose as features (from stemmed or original text) and why?\", explain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe12931-f033-4b17-ad2d-6913a35c0d78",
   "metadata": {},
   "source": [
    "### Excercise 6: Text analysis\n",
    "\n",
    "Here, we want to see if there is a correlation between the `tf-idf` of each word and the sentiment labels. Fill in the function below to get the first `30` words with the most `tf-idf` values, for the positive and negative comments separately. Do you see a correlation between the words used and the sentiment labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "31f401f0-ae53-488b-a786-51b412347df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = train_data.review.values\n",
    "tf_idf = vectorizer.fit_transform(corpus).toarray()[0]\n",
    "names = vectorizer.get_feature_names_out()\n",
    "words_dict = {names[i] : tf_idf[i] for i in range(len(tf_idf))}    \n",
    "words_dict = dict(sorted(words_dict.items(), key=lambda item: item[1], reverse = True))\n",
    "\n",
    "#print(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a02ff439-d21b-4062-9572-87eb631d5339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive words with the most tf-idf values: {'adorée': 0.417481649145574, 'regardée': 0.36829080016728616, 'ai': 0.3433314805725262, 'jeune': 0.33235176900009583, 'étais': 0.313458998343722, 'était': 0.25651583123087446, 'première': 0.2541458635237441, 'quand': 0.2313854105183392, 'géniale': 0.2174966565675356, 'elle': 0.1745403941419963, 'vraiment': 0.16332047347332052, 'plus': 0.1528977545179396, 'je': 0.127756362219467, 'que': 0.11419906965500506, 'la': 0.09413120728574602, 'est': 0.08718672062142567, 'série': 0.07684232254845959, '04': 0.0, '05': 0.0, '06': 0.0, '10': 0.0, '100': 0.0, '11': 0.0, '11h30': 0.0, '12': 0.0, '12e': 0.0, '13': 0.0, '14': 0.0, '15': 0.0, '16': 0.0} \n",
      "\n",
      "negative words with the most tf-idf values: {'rentre': 0.506374348708167, 'inintéressante': 0.39709213038023883, 'banale': 0.38471233988225617, 'décevant': 0.3535096995319206, 'scénario': 0.22893304325533018, 'série': 0.20822457072367626, 'dans': 0.193340674398364, 'tout': 0.17962289262825945, 'du': 0.17718307830124075, 'on': 0.16283500912105614, 'ne': 0.15490933051577252, 'le': 0.12364614548021885, 'la': 0.1188566537691992, 'pas': 0.11696576234199572, 'et': 0.10822874931873089, 'est': 0.10393097454656863, '00': 0.0, '0001': 0.0, '02x02': 0.0, '09': 0.0, '10': 0.0, '100': 0.0, '1000': 0.0, '12': 0.0, '13': 0.0, '14': 0.0, '14ème': 0.0, '15': 0.0, '15j': 0.0, '16': 0.0} \n",
      "\n",
      "Do you see a correlation of the words with the most tf-idf values, and sentiment labels? we can see that the words with the highest tf-idf values for the positive reviews are words with positive meaning (adorée, géniale...) whereas the opposite is true for the negative reviews, this correlation could help determine wether a review is positive or not, although we have quite a bit of noise since there's a big number of neutral terms with high tf-idf values \n"
     ]
    }
   ],
   "source": [
    "def get_most_repeated(data, vectorizer):\n",
    "    corpus = data.review.values\n",
    "    tf_idf = vectorizer.fit_transform(corpus).toarray()[0]\n",
    "    names = vectorizer.get_feature_names_out()\n",
    "    words_dict = {names[i] : tf_idf[i] for i in range(len(tf_idf))}\n",
    "    words_dict = dict(sorted(words_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    i = 0\n",
    "    words = {}\n",
    "    for word in words_dict:\n",
    "        words[word] = words_dict[word]\n",
    "        i += 1\n",
    "        if i == 30 :\n",
    "            break\n",
    "    return words\n",
    "    \n",
    "positive_words = get_most_repeated(train_data_positives, vectorizer)\n",
    "negative_words = get_most_repeated(train_data_negatives, vectorizer)\n",
    "print(\"positive words with the most tf-idf values:\", positive_words, '\\n')\n",
    "print(\"negative words with the most tf-idf values:\", negative_words, '\\n')\n",
    "\n",
    "explain = \"we can see that the words with the highest tf-idf values for the positive reviews are words with positive meaning (adorée, géniale...) whereas the opposite is true for the negative reviews, this correlation could help determine wether a review is positive or not, although we have quite a bit of noise since there's a big number of neutral terms with high tf-idf values \"\n",
    "print(\"Do you see a correlation of the words with the most tf-idf values, and sentiment labels?\", explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a43178-f529-4199-b529-4a9b830ed85a",
   "metadata": {},
   "source": [
    "## Acoustic signal analysis with Python\n",
    "\n",
    "Acoustic signal analysis is a powerful tool used to extract meaningful information from acoustic recordings. Here, we cover the following topics:\n",
    "\n",
    "- Different audio encodings and conversion\n",
    "- Analysing acoustic recordings with spectrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45e90f-89a0-4684-b3d5-f7b7652b7f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading two audio recordings to analyse\n",
    "dl_path = 'https://cdn.pixabay.com/download/audio/2023/01/09/audio_baaa3cfec7.mp3?filename=acoustic-guitar-loop-f-91bpm-132687.mp3'\n",
    "os.system(f\"curl {dl_path} --compressed -o guitar.mp3\")\n",
    "dl_path = \"https://cdn.pixabay.com/download/audio/2022/03/15/audio_3e683188e5.mp3?filename=tooheavy-77056.mp3\"\n",
    "os.system(f\"curl {dl_path} --compressed -o speech.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b58e9-3e6a-4739-af0e-1401a522cbb9",
   "metadata": {},
   "source": [
    "### Excercise 7: Audio encodings\n",
    "\n",
    "Below, write a script to convert the `.mp3` files to `.wav` files formatted as `PCM signed 16-bit little-endian`. We would also like to have the file as `mono`, so that they only contain one channel. Last but not the least, change the sampling frequency of the files to `16000 Hz`. After writting the script, explain the questions asked in the code section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f633f6f2-3b51-4b3b-99bd-7c63da1f70b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference between mp3, and PCM formatted wav file? ...\n",
      "What is the sampling frequency, its difference with bit-rate? ...\n",
      "Analysing speech signals, and building machine learning models to recognise human speech are often done with 16KHz sampled audio files. Why do you think that is the case ? ...\n"
     ]
    }
   ],
   "source": [
    "def convert_audio(input_path, output_path):\n",
    "    args = 'Write your code here'\n",
    "    os.system(f'ffmpeg -i {input_path} {args} -y {output_path}')\n",
    "\n",
    "convert_audio(\"guitar.mp3\", \"guitar_converted.wav\")\n",
    "convert_audio(\"speech.mp3\", \"speech_converted.wav\")\n",
    "\n",
    "explain = \"MP3 files use lossy compression algorithms to remove parts of the audio that are less noticeable to the human ear which gives them a smaller size but lower quality and dynamic range. PCM formatted WAV files, on the other hand, are uncompressed audio files that retain all the audio data in the original recording\"\n",
    "print(\"What is the difference between mp3, and PCM formatted wav file?\", explain)\n",
    "\n",
    "explain = \"sampling frequency : number of samples of audio data taken per second | bitrate = the number of bits used to represent each sample in the audio file\"\n",
    "print(\"What is the sampling frequency, its difference with bit-rate?\", explain)\n",
    "\n",
    "explain = \"A sampling frequency of 16 kHz is commonly used for speech signal analysis and machine learning models for speech recognition because it provides a high level of detail in the signal while still keeping the file size manageable. At 16 kHz, the Nyquist frequency, which is the highest frequency that can be accurately represented by the digital audio, is 8 kHz, which is well above the frequency range of human speech. This means that the audio can be accurately captured and analyzed without losing any important information.\"\n",
    "print(\"Analysing speech signals, and building machine learning models to recognise human speech are often done with 16KHz sampled audio files. Why do you think that is the case ?\", explain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaaa064-41b9-4ba7-9fd4-8f44bbcc1d1f",
   "metadata": {},
   "source": [
    "### Excercise 8: Normalisation\n",
    "\n",
    "It is a common practice to normalise the wav signal to contain values between zero and one. Write a very short script (can be one line) to normalise the wav file after `reading` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d6227-9e9c-4c59-ae03-472d53307692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "\n",
    "def normalise_signal(sig):\n",
    "    '''\n",
    "    Write your code here\n",
    "    '''\n",
    "    return sig_normed\n",
    "\n",
    "(rate,sig) = wav.read(\"guitar_converted.wav\")\n",
    "sig_normed = normalise_signal(sig)\n",
    "print(\"original signal values:\", sig[995:1000])\n",
    "print(\"normalised signal values\", sig_normed[995:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c2ccc-bf9c-4c60-a5ef-7139631e67ba",
   "metadata": {},
   "source": [
    "### Excercise 9: Spectrograms\n",
    "\n",
    "Below, we use the `matplotlib` package to plot the spectrograms of the two acoustic signals (we plot only the first three seconds for the image to be more clear). A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time (from [wikipedia](https://en.wikipedia.org/wiki/Spectrogram)). After running the code below to plot the spectrograms, answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0d3560d-fefa-4907-b318-e4b868130af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m (rate,sig) \u001b[38;5;241m=\u001b[39m \u001b[43mwav\u001b[49m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguitar_converted.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m spc, f, t, img \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mspecgram(sig[:rate\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m], Fs\u001b[38;5;241m=\u001b[39mrate, NFFT\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m) \u001b[38;5;66;03m#spc, f, t, img\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wav' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(rate,sig) = wav.read(\"guitar_converted.wav\")\n",
    "spc, f, t, img = plt.specgram(sig[:rate*3], Fs=rate, NFFT=512) #spc, f, t, img\n",
    "plt.show()\n",
    "\n",
    "(rate,sig) = wav.read(\"speech_converted.wav\")\n",
    "spc, f, t, img = plt.specgram(sig[:rate*3], Fs=rate, NFFT=512) #spc, f, t, img\n",
    "plt.show()\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"From the spectrograms, how can you tell which one is human speech, and which one is guitar sound?\", explain)\n",
    "\n",
    "explain = \"...\"\n",
    "print(\"Try changing the value of NFFT, what change do you observe? why is that?\", explain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a38e0-bc1f-4ad1-afad-55958e3c489e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this practical work, the basics of acoustic and textual data analysis with Python have been explored. First, the fundamentals of Python were covered, focusing on classes and how to write clean codes. Then, the basics of text analysis were discussed, including downloading datasets and extracting basic features such as `tf-idf`. Finally, acoustic signal analysis with Python was discussed, focusing on audio encodings, conversion of the encodings, and acoustic analysis of different sounds with `spectrograms`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
